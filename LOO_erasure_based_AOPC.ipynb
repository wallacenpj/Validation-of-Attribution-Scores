{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBoCFXDBPK9j"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Erasure-Based (Leave-one-out [LOO]) AOPC Calculation for BERT Sequence Classification Models\n",
        "==================================================================\n",
        "This script demonstrates how to compute LOO attributions and AOPC (Area Over the Perturbation Curve)\n",
        "for a HuggingFace Transformer model. LOO helps explain predictions by identifying which parts of text\n",
        "are most important for a model's decision.\n",
        "The script is designed to be run locally or on a hosted runtime like Colab.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "-------------\n",
        "1. Install requirements (see below).\n",
        "2. Set your own HuggingFace model and tokenizer, and provide paths to your train/test data.\n",
        "3. Run the script!\n",
        "\n",
        "REQUIREMENTS:\n",
        "-------------\n",
        "!pip install transformers datasets pandas torch\n",
        "\n",
        "If running in Colab, uncomment and run the pip commands at the top of your notebook.\n",
        "\"\"\"\n",
        "\n",
        "!pip install transformers datasets pandas torch\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import re\n",
        "\n",
        "# Import required libraries for model loading\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# To print NumPy scalars as Python scalars, i.e., without \"np.float64\"\n",
        "np.set_printoptions(legacy='1.25')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Login to your HuggingFace Hub account"
      ],
      "metadata": {
        "id": "V7GPt1Ce-hm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"your_token\") # <-- CHANGE THIS to your HuggingFace Login Access Token"
      ],
      "metadata": {
        "id": "jy9lsmg5Unwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Check GPU and RAM availability"
      ],
      "metadata": {
        "id": "D8ulwN75PsST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Optional: Check GPU and RAM availability --\n",
        "def print_gpu_ram_info():\n",
        "    try:\n",
        "        import subprocess\n",
        "        # Check GPU info\n",
        "        gpu_info = subprocess.check_output(['nvidia-smi']).decode()\n",
        "        print(\"GPU Info:\\n\", gpu_info)\n",
        "    except Exception:\n",
        "        print('No GPU found or not connected to a GPU.')\n",
        "\n",
        "    # Check RAM info\n",
        "    try:\n",
        "        from psutil import virtual_memory\n",
        "        ram_gb = virtual_memory().total / 1e9\n",
        "        print('Your runtime has {:.1f} GB of available RAM\\n'.format(ram_gb))\n",
        "    except ImportError:\n",
        "        print('psutil not installed, skipping RAM check.')\n",
        "\n",
        "# Call the function (optional)\n",
        "print_gpu_ram_info()"
      ],
      "metadata": {
        "id": "KsoFOqAQPoQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "User Configuration"
      ],
      "metadata": {
        "id": "YFBTIpf9Q_uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User Configuration ---\n",
        "# Provide the name of your model (must be compatible with HuggingFace Transformers)\n",
        "MODEL_NAME = \"your_model\"  # <-- CHANGE THIS to your model\n",
        "\n",
        "# Provide the name of your tokenizer\n",
        "TOKENIZER = \"your_tokenizer\" # <-- CHANGE THIS if not the same as your MODEL_NAME\n",
        "\n",
        "# Path to your test CSV file (should have at least 'EssayText' and 'essay_score' columns)\n",
        "TEST_CSV_PATH = \"path/to/your/test_data.csv\"  # <-- CHANGE THIS to your test data path\n",
        "\n",
        "# Number of classes in your classification problem\n",
        "NUM_LABELS = 4  # <-- CHANGE THIS to your number of classes\n",
        "\n",
        "# Class names (must match your dataset)\n",
        "CLASS_NAMES = [str(i) for i in range(NUM_LABELS)]  # or use your actual class names\n",
        "\n",
        "# Random seed for reproducibility\n",
        "RANDOM_STATE = 0"
      ],
      "metadata": {
        "id": "bQSiAWu5Pqmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model and Tokenizer"
      ],
      "metadata": {
        "id": "KYEdlT1qRB5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Model and Tokenizer ---\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER)\n",
        "\n",
        "# Use GPU if available, else CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "UAJd39qwRCZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "9GNyr27CRKyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Data ---\n",
        "# If using HuggingFace datasets:\n",
        "# test_set = load_dataset('csv', data_files=TEST_CSV_PATH)['train']\n",
        "# test_doc = list(test_set['EssayText'])\n",
        "\n",
        "# Or load with pandas:\n",
        "test = pd.read_csv(TEST_CSV_PATH)\n",
        "assert 'EssayText' in test.columns, \"Your CSV file must have an 'EssayText' column.\"\n",
        "test_doc = list(test['EssayText'])\n",
        "test.head()"
      ],
      "metadata": {
        "id": "f1KukNjsRJdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Functions to Predict Class Probabilities, Compute LOO Attribution Scores, and Compute the AOPC"
      ],
      "metadata": {
        "id": "DIT8-axFWkDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions ---\n",
        "\n",
        "def predict(text):\n",
        "    \"\"\"\n",
        "    Generates the model's prediction probabilities for the given text.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    # Perform forward pass with the inputs on the same device\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the logits from the model outputs\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Apply sigmoid to the logits (for binary classification)\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "    # Move the tensor to the CPU before converting to NumPy\n",
        "    probabilities = probabilities.detach().cpu().numpy()\n",
        "\n",
        "    # Return the probabilities\n",
        "    return probabilities\n",
        "\n",
        "def erasure_attribution(text):\n",
        "    \"\"\"\n",
        "    Computes attribution scores using the erasure method.\n",
        "    \"\"\"\n",
        "    tokens = re.split(r'\\W+', text)\n",
        "\n",
        "    original_prediction = predict(text)\n",
        "\n",
        "    # Get the predicted class (the one with the highest score)\n",
        "    predicted_class_idx = np.argmax(original_prediction)\n",
        "\n",
        "    attributions = []\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "        # Erase (remove) the token at position i\n",
        "        perturbed_tokens = tokens[:i] + tokens[i+1:]\n",
        "        perturbed_text = \" \".join(perturbed_tokens)\n",
        "\n",
        "        # Get the model's prediction for the perturbed text\n",
        "        perturbed_prediction = predict(perturbed_text)\n",
        "\n",
        "        # Calculate the change in prediction only for the predicted class\n",
        "        score_change = original_prediction[0, predicted_class_idx] - perturbed_prediction[0, predicted_class_idx]\n",
        "        attributions.append(score_change)\n",
        "\n",
        "    # Create a dictionary of tokens and their respective unnormalized attributions\n",
        "    unnormalized_token_attributions = dict(zip(tokens, attributions))\n",
        "    print(\"Unnormalized attributions\", unnormalized_token_attributions, ' \\n')\n",
        "\n",
        "    # Normalize the attribution scores for the predicted class\n",
        "    max_score_change = np.max(attributions) if attributions else 1.0  # Avoid division by zero\n",
        "    attributions = [score / max_score_change for score in attributions]\n",
        "\n",
        "\n",
        "    # Create a dictionary of tokens and their respective normalized attributions\n",
        "    normalized_token_attributions = dict(zip(tokens, attributions))\n",
        "    print(\"Normalized attributions\", normalized_token_attributions, ' \\n')\n",
        "\n",
        "    return unnormalized_token_attributions, np.array(attributions), tokens, predicted_class_idx\n",
        "\n",
        "def compute_aopc_erasure(text):\n",
        "    \"\"\"\n",
        "    Computes the Area Over the Perturbation Curve (AOPC) for a given text based on the erasure method.\n",
        "    Calculates score drops for the predicted class only.\n",
        "    \"\"\"\n",
        "    # Obtain erasure-based attribution scores\n",
        "    unnormalized_token_attributions, attribution_scores, tokens, predicted_class_idx = erasure_attribution(text)\n",
        "\n",
        "    # Obtain the predicted class (the one with the highest probability)\n",
        "    original_prediction = predict(text)\n",
        "\n",
        "    # Sort tokens by their importance for the predicted class based on attribution scores\n",
        "    sorted_indices = np.argsort(-attribution_scores)\n",
        "\n",
        "    # Determine the top 20% of tokens to remove\n",
        "    n_top_tokens = max(1, int(0.2 * len(sorted_indices)))  # Ensure at least one token is perturbed\n",
        "    top_indices_to_remove = sorted_indices[:n_top_tokens]\n",
        "\n",
        "    total_score_drop = 0\n",
        "\n",
        "    for i in range(1, n_top_tokens + 1):\n",
        "        # Perturb the input by removing the top i important tokens\n",
        "        indices_to_remove = top_indices_to_remove[:i]\n",
        "        print('removed: ' ,indices_to_remove)\n",
        "        perturbed_tokens = [tokens[j] for j in range(len(tokens)) if j not in indices_to_remove]\n",
        "        perturbed_text = \" \".join(perturbed_tokens)\n",
        "        print('perturbed_text: ', perturbed_text)\n",
        "\n",
        "        # Get new prediction for perturbed text\n",
        "        new_prediction = predict(perturbed_text)\n",
        "\n",
        "        # Calculate the drop in prediction score for the predicted class\n",
        "        score_drop = original_prediction[0, predicted_class_idx] - new_prediction[0, predicted_class_idx]\n",
        "        total_score_drop += score_drop\n",
        "        print(score_drop)\n",
        "\n",
        "    # Calculate AOPC for the predicted class using only the top 20% of important tokens\n",
        "    aopc = total_score_drop / n_top_tokens\n",
        "    return unnormalized_token_attributions, aopc"
      ],
      "metadata": {
        "id": "AcLeHSDlRP_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Loop: Calculate AOPC for Test Set, Export Results to a CSV file, and Print Group-Wise Mean AOPC by Score"
      ],
      "metadata": {
        "id": "8_mFyk4Z7qAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Loop: Calculate AOPC for Test Set ---\n",
        "\n",
        "aopc_list = []\n",
        "attributions_scores_list = []\n",
        "for idx, ex in enumerate(test_doc):\n",
        "    print(f\"Processing example {idx+1} of {len(test_doc)}\")\n",
        "    attributions_scores, aopc = compute_aopc_erasure(ex)\n",
        "    attributions_scores_list.append(attributions_scores)\n",
        "    aopc_list.append(aopc)\n",
        "\n",
        "# Report Mean AOPC\n",
        "print('Average AOPC to report: ', np.mean(aopc_list))\n",
        "\n",
        "# --- Save Results ---\n",
        "# Make sure 'essay_score' column exists in your CSV.\n",
        "output_df = pd.DataFrame({\n",
        "    'response': test_doc,\n",
        "    'attributions_scores': attributions_scores_list,\n",
        "    'loo_aopc': aopc_list,\n",
        "    'original_score': test['essay_score'] if 'essay_score' in test.columns else np.nan\n",
        "})\n",
        "\n",
        "output_df.to_csv('loo_aopc.csv', index=False)\n",
        "\n",
        "# Print group-wise mean AOPC by score (if available)\n",
        "if 'essay_score' in test.columns:\n",
        "    print(output_df.groupby('original_score')['loo_aopc'].mean())"
      ],
      "metadata": {
        "id": "7Wh88yGIWzt_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
