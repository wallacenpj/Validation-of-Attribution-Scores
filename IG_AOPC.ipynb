{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDWbL-bcS4pr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Integrated Gradients (IG) AOPC Calculation for BERT Sequence Classification Models\n",
        "==================================================================\n",
        "This script demonstrates how to compute IG attributions and AOPC (Area Over the Perturbation Curve)\n",
        "for a HuggingFace Transformer model. IG helps explain predictions by identifying which parts of text\n",
        "are most important for a model's decision.\n",
        "The script is designed to be run locally or on a hosted runtime like Colab.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "-------------\n",
        "1. Install requirements (see below).\n",
        "2. Set your own HuggingFace model and tokenizer, and provide paths to your train/test data.\n",
        "3. Run the script!\n",
        "\n",
        "CAUTION:\n",
        "-------------\n",
        "Installing transformers_interpret in the first notebook cell may cause dependency conflicts with numpy and pandas.\n",
        "This can result in import errors or binary incompatibility issues.\n",
        "\n",
        "Installing transformers_interpret in the order provided in this notebook helps avoid version conflicts and ensures stable imports.\n",
        "If running in Colab, IGNORE THE MESSAGE TO RESTART THE RUNTIME after transformers_interpret is installed.\n",
        "\"\"\"\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# To print NumPy scalars as Python scalars, i.e., without \"np.float64\"\n",
        "#np.set_printoptions(legacy='1.25')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Login to your HuggingFace Hub account"
      ],
      "metadata": {
        "id": "REEgqCeCTaJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"your_token\") # <-- CHANGE THIS to your HuggingFace Login Access Token"
      ],
      "metadata": {
        "id": "xRTVE5cXTYMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Check GPU and RAM availability"
      ],
      "metadata": {
        "id": "St7gO238TcH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Optional: Check GPU and RAM availability --\n",
        "def print_gpu_ram_info():\n",
        "    try:\n",
        "        import subprocess\n",
        "        # Check GPU info\n",
        "        gpu_info = subprocess.check_output(['nvidia-smi']).decode()\n",
        "        print(\"GPU Info:\\n\", gpu_info)\n",
        "    except Exception:\n",
        "        print('No GPU found or not connected to a GPU.')\n",
        "\n",
        "    # Check RAM info\n",
        "    try:\n",
        "        from psutil import virtual_memory\n",
        "        ram_gb = virtual_memory().total / 1e9\n",
        "        print('Your runtime has {:.1f} GB of available RAM\\n'.format(ram_gb))\n",
        "    except ImportError:\n",
        "        print('psutil not installed, skipping RAM check.')\n",
        "\n",
        "# Call the function (optional)\n",
        "print_gpu_ram_info()"
      ],
      "metadata": {
        "id": "DWJEvd2-TYY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "User Configuration"
      ],
      "metadata": {
        "id": "lkpoOYWjTgJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User Configuration ---\n",
        "# Provide the name of your model (must be compatible with HuggingFace Transformers)\n",
        "MODEL_NAME = \"your_model\"  # <-- CHANGE THIS to your model\n",
        "\n",
        "# Provide the name of your tokenizer\n",
        "TOKENIZER = \"your_tokenizer\" # <-- CHANGE THIS if not the same as your MODEL_NAME\n",
        "\n",
        "# Path to your test CSV file (should have at least 'EssayText' and 'essay_score' columns)\n",
        "TEST_CSV_PATH = \"path/to/your/test_data.csv\"  # <-- CHANGE THIS to your test data path\n",
        "\n",
        "# Number of classes in your classification problem\n",
        "NUM_LABELS = 4  # <-- CHANGE THIS to your number of classes\n",
        "\n",
        "# Class names (must match your dataset)\n",
        "CLASS_NAMES = [str(i) for i in range(NUM_LABELS)]  # or use your actual class names\n",
        "\n",
        "# Random seed for reproducibility\n",
        "RANDOM_STATE = 0\n",
        "\n",
        "# Number of steps to compute IG\n",
        "# N_STEPS = 50 # <-- The default is 50"
      ],
      "metadata": {
        "id": "b-8OATMgTgwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "6m6l0vi9UTm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Data ---\n",
        "# If using HuggingFace datasets:\n",
        "# test_set = load_dataset('csv', data_files=TEST_CSV_PATH)['train']\n",
        "# test_doc = list(test_set['EssayText'])\n",
        "\n",
        "# Or load with pandas:\n",
        "test = pd.read_csv(TEST_CSV_PATH)\n",
        "assert 'EssayText' in test.columns, \"Your CSV file must have an 'EssayText' column.\"\n",
        "test_doc = list(test['EssayText'])\n",
        "test.head()"
      ],
      "metadata": {
        "id": "gV08SWRWUVLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model and Tokenizer"
      ],
      "metadata": {
        "id": "YH5LAeS5T_O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Model and Tokenizer ---\n",
        "# Import required libraries for model loading\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER)\n",
        "\n",
        "# Use GPU if available, else CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "nwGC_Co8T-sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Functions to Initialize the IG explainer to Compute IG Attribution Scores, Perturb the Text, and Compute the AOPC"
      ],
      "metadata": {
        "id": "QiR0aNiZUVYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Caution:\n",
        "Installing transformers_interpret in the first notebook cell may cause dependency conflicts with numpy and pandas.\n",
        "This can result in import errors or binary incompatibility issues.\n",
        "\n",
        "Installing transformers_interpret in this cell helps avoid version conflicts and ensures stable imports.\n",
        "If running in Colab, IGNORE THE MESSAGE TO RESTART THE RUNTIME after transformers_interpret is installed.\n",
        "\"\"\"\n",
        "\n",
        "!pip install transformers_interpret"
      ],
      "metadata": {
        "id": "cT2SdE5k8KCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers_interpret import SequenceClassificationExplainer\n",
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "# Initialize IG explainer\n",
        "explainer = SequenceClassificationExplainer(model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Pipeline to Predict the Probabilities\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
        "\n",
        "# Move the pipeline to the GPU\n",
        "pipe.device = device\n",
        "\n",
        "def perturb_text(tokens, indices_to_remove):\n",
        "    \"\"\"\n",
        "    Removes or masks the tokens at the specified indices.\n",
        "    \"\"\"\n",
        "    for idx in sorted(indices_to_remove, reverse=True):\n",
        "        tokens[idx] = \"\"  # You can use \"[PAD]\" or \"\" (empty string) to remove\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def compute_aopc_ig(text):\n",
        "    \"\"\"\n",
        "    Computes the Area Over the Perturbation Curve (AOPC) for a given text, considering score drops for the predicted class only.\n",
        "    \"\"\"\n",
        "    # Get original prediction and attribution scores\n",
        "    attributions = explainer(text) # Different from LOO and LIME, IG attributions are stored as a list because repeated tokens have different attribution scores\n",
        "    print(\"Attributions:\", attributions)\n",
        "    original_prediction_scores = pipe(text)[0]\n",
        "\n",
        "    # Identify the predicted class (the one with the highest score)\n",
        "    predicted_class_idx = np.argmax([score['score'] for score in original_prediction_scores])\n",
        "\n",
        "    # Extract score for the predicted class\n",
        "    original_prediction = original_prediction_scores[predicted_class_idx]['score']\n",
        "\n",
        "    # Sort attributions by importance (score) in descending order\n",
        "    sorted_attributions = sorted(attributions, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Tokenize the text using tokenizer\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Create a mapping from tokens to their positions\n",
        "    token_positions = {token: idx for idx, token in enumerate(tokens)}\n",
        "\n",
        "    # Find the indices of the tokens to remove based on sorted attributions\n",
        "    indices_to_remove = []\n",
        "    for token, _ in sorted_attributions:\n",
        "        if token in token_positions:\n",
        "            indices_to_remove.append(token_positions[token])\n",
        "\n",
        "    # Only consider the top 20% of tokens\n",
        "    n_top_tokens = max(1, int(0.2 * len(indices_to_remove)))  # Ensure at least one token is perturbed\n",
        "    top_indices_to_remove = indices_to_remove[:n_top_tokens]\n",
        "\n",
        "    # Initialize the total score drop for the predicted class\n",
        "    total_score_drop = 0\n",
        "\n",
        "    for i in range(1, n_top_tokens + 1):\n",
        "        # Perturb the input by removing top i important tokens\n",
        "        current_indices_to_remove = top_indices_to_remove[:i]\n",
        "        perturbed_text = perturb_text(tokens.copy(), current_indices_to_remove)\n",
        "        print(\"Perturbed text: \", perturbed_text)\n",
        "        print(\"Current indices to remove:\", current_indices_to_remove)\n",
        "\n",
        "        # Get new prediction scores for perturbed text using pipe\n",
        "        new_prediction_scores = pipe(perturbed_text)[0]\n",
        "\n",
        "        # Get the new score for the predicted class\n",
        "        new_prediction = new_prediction_scores[predicted_class_idx]['score']\n",
        "\n",
        "        # Calculate the drop in prediction score for the predicted class\n",
        "        score_drop = original_prediction - new_prediction\n",
        "        total_score_drop += score_drop\n",
        "        print(\"Score drop:\", score_drop)\n",
        "\n",
        "    # Calculate AOPC for the predicted class using only the top 20% of important tokens\n",
        "    aopc = total_score_drop / n_top_tokens\n",
        "    return attributions, aopc"
      ],
      "metadata": {
        "id": "Pjg0MyGKTYb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Loop: Calculate AOPC for Test Set, Export Results to a CSV file, and Print Group-Wise Mean AOPC by Score"
      ],
      "metadata": {
        "id": "YE0krSs2VYa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Loop: Calculate AOPC for Test Set ---\n",
        "\n",
        "aopc_list = []\n",
        "attribution_scores_list = []\n",
        "for idx, ex in enumerate(test_doc):\n",
        "    print(f\"Processing example {idx+1} of {len(test_doc)}\")\n",
        "    attribution_scores, aopc = compute_aopc_ig(ex)\n",
        "    attribution_scores_list.append(attribution_scores)\n",
        "    aopc_list.append(aopc)\n",
        "\n",
        "# Report Mean AOPC\n",
        "print('Average AOPC to report: ', np.mean(aopc_list))\n",
        "\n",
        "# --- Save Results ---\n",
        "# Make sure 'essay_score' column exists in your CSV.\n",
        "output_df = pd.DataFrame({\n",
        "    'response': test_doc,\n",
        "    'attributions_scores': attribution_scores_list,\n",
        "    'ig_aopc': aopc_list,\n",
        "    'original_score': test['essay_score'] if 'essay_score' in test.columns else np.nan\n",
        "})\n",
        "\n",
        "output_df.to_csv('ig_aopc.csv', index=False)\n",
        "\n",
        "# Print group-wise mean AOPC by score (if available)\n",
        "if 'essay_score' in test.columns:\n",
        "    print(output_df.groupby('original_score')['ig_aopc'].mean())"
      ],
      "metadata": {
        "id": "GLNhNq2MTYfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}