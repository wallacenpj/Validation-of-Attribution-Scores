{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBoCFXDBPK9j"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "LIME-Based AOPC Calculation for BERT Sequence Classification Models\n",
        "==================================================================\n",
        "This script demonstrates how to compute LIME attributions and AOPC (Area Over the Perturbation Curve)\n",
        "for a HuggingFace Transformer model. LIME helps explain predictions by identifying which parts of text\n",
        "are most important for a model's decision.\n",
        "The script is designed to be run locally or on a hosted runtime like Colab.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "-------------\n",
        "1. Install requirements (see below).\n",
        "2. Set your own HuggingFace model and tokenizer, and provide paths to your train/test data.\n",
        "3. Run the script!\n",
        "\n",
        "REQUIREMENTS:\n",
        "-------------\n",
        "!pip install transformers datasets lime pandas torch\n",
        "\n",
        "If running in Colab, uncomment and run the pip commands at the top of your notebook.\n",
        "\"\"\"\n",
        "\n",
        "!pip install transformers datasets lime pandas torch\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import re\n",
        "\n",
        "# Import required libraries for model loading\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Import required libraries for model interpretation\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# To print NumPy scalars as Python scalars, i.e., without \"np.float64\"\n",
        "np.set_printoptions(legacy='1.25')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Login to your HuggingFace Hub account"
      ],
      "metadata": {
        "id": "V7GPt1Ce-hm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"your_token\") # <-- CHANGE THIS to your HuggingFace Login Access Token"
      ],
      "metadata": {
        "id": "jy9lsmg5Unwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Check GPU and RAM availability"
      ],
      "metadata": {
        "id": "D8ulwN75PsST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Optional: Check GPU and RAM availability --\n",
        "def print_gpu_ram_info():\n",
        "    try:\n",
        "        import subprocess\n",
        "        # Check GPU info\n",
        "        gpu_info = subprocess.check_output(['nvidia-smi']).decode()\n",
        "        print(\"GPU Info:\\n\", gpu_info)\n",
        "    except Exception:\n",
        "        print('No GPU found or not connected to a GPU.')\n",
        "\n",
        "    # Check RAM info\n",
        "    try:\n",
        "        from psutil import virtual_memory\n",
        "        ram_gb = virtual_memory().total / 1e9\n",
        "        print('Your runtime has {:.1f} GB of available RAM\\n'.format(ram_gb))\n",
        "    except ImportError:\n",
        "        print('psutil not installed, skipping RAM check.')\n",
        "\n",
        "# Call the function (optional)\n",
        "print_gpu_ram_info()\n",
        "\n"
      ],
      "metadata": {
        "id": "KsoFOqAQPoQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "User Configuration"
      ],
      "metadata": {
        "id": "YFBTIpf9Q_uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User Configuration ---\n",
        "# Provide the name of your model (must be compatible with HuggingFace Transformers)\n",
        "MODEL_NAME = \"your_model\"  # <-- CHANGE THIS to your model\n",
        "\n",
        "# Provide the name of your tokenizer\n",
        "TOKENIZER = \"your_tokenizer\" # <-- CHANGE THIS if not the same as your MODEL_NAME\n",
        "\n",
        "# Path to your test CSV file (should have at least 'EssayText' and 'essay_score' columns)\n",
        "TEST_CSV_PATH = \"path/to/your/test_data.csv\"  # <-- CHANGE THIS to your test data path\n",
        "\n",
        "# Number of classes in your classification problem\n",
        "NUM_LABELS = 2  # <-- CHANGE THIS to your number of classes\n",
        "\n",
        "# Class names (must match your dataset)\n",
        "CLASS_NAMES = [str(i) for i in range(NUM_LABELS)]  # or use your actual class names\n",
        "\n",
        "# Random seed for reproducibility\n",
        "RANDOM_STATE = 0\n",
        "\n",
        "# Number of features to compute LIME\n",
        "NUM_FEATURES = 10 # <-- The default is 10\n",
        "\n",
        "# Number of samples used by LIME to explain predictions\n",
        "NUM_SAMPLES = 100 # <-- The default is 100"
      ],
      "metadata": {
        "id": "bQSiAWu5Pqmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model and Tokenizer"
      ],
      "metadata": {
        "id": "KYEdlT1qRB5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Model and Tokenizer ---\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER)\n",
        "\n",
        "# Use GPU if available, else CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "UAJd39qwRCZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "9GNyr27CRKyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Data ---\n",
        "# If using HuggingFace datasets:\n",
        "# test_set = load_dataset('csv', data_files=TEST_CSV_PATH)['train']\n",
        "# test_doc = list(test_set['EssayText'])\n",
        "\n",
        "# Or load with pandas:\n",
        "test = pd.read_csv(TEST_CSV_PATH)\n",
        "assert 'EssayText' in test.columns, \"Your CSV file must have an 'EssayText' column.\"\n",
        "test_doc = list(test['EssayText'][0:5])\n",
        "test.head()"
      ],
      "metadata": {
        "id": "f1KukNjsRJdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Functions to Predict Class Probabilities, Compute LIME Attribution Scores, and Compute the AOPC"
      ],
      "metadata": {
        "id": "DIT8-axFWkDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions ---\n",
        "\n",
        "def predict_proba(texts):\n",
        "    \"\"\"\n",
        "    Predict class probabilities for a list of texts.\n",
        "    Uses small batch size to reduce memory usage.\n",
        "    \"\"\"\n",
        "    batch_size = 1  # Adjust if you have enough RAM/GPU\n",
        "    all_probs = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "        with torch.no_grad():  # Disable gradient calculation to save memory\n",
        "            outputs = model(**inputs)\n",
        "        probs = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        all_probs.append(probs)\n",
        "    return np.vstack(all_probs)\n",
        "\n",
        "def lime_attribution(text):\n",
        "    \"\"\"\n",
        "    Compute LIME attribution scores for the given text.\n",
        "    Returns (attributions, tokens).\n",
        "    \"\"\"\n",
        "    explainer = LimeTextExplainer(class_names=[0,1,2,3], random_state=RANDOM_STATE)\n",
        "    exp = explainer.explain_instance(text, predict_proba, num_features=NUM_FEATURES, num_samples=NUM_SAMPLES)\n",
        "    importance_scores = dict(exp.as_list()) #Here the tokens will be sorted from highest to lowest attribution score\n",
        "\n",
        "    # Tokenize the text as LIME does (word-level, removing punctuation)\n",
        "    tokens = re.split(r'\\W+', text)\n",
        "\n",
        "    # Match tokens with their importance scores to pass to the compute_aopc_lime function\n",
        "    attributions = np.array([importance_scores.get(token, 0) for token in tokens])\n",
        "    return attributions, tokens\n",
        "\n",
        "def compute_aopc_lime(text):\n",
        "    \"\"\"\n",
        "    For a given text, computes the AOPC (Area Over the Perturbation Curve)\n",
        "    using LIME attributions.\n",
        "    \"\"\"\n",
        "    # Obtain LIME attribution scores\n",
        "    lime_values, tokens = lime_attribution(text)\n",
        "    attributions_scores = dict(zip(tokens, lime_values))\n",
        "    print(\"Attribution scores: \", attributions_scores)\n",
        "\n",
        "    # Sort tokens by their importance for the predicted class based on LIME attribution scores (descending\n",
        "\n",
        "    # Obtain the predicted class (the one with the highest probability)\n",
        "    original_prediction = predict_proba([text])[0]\n",
        "    predicted_class_idx = np.argmax(original_prediction)\n",
        "\n",
        "    # Sort tokens by their importance for the predicted class based on LIME attribution scores (descending)\n",
        "    sorted_indices = np.argsort(-lime_values)\n",
        "\n",
        "    # Determine the top 20% of tokens to remove\n",
        "    n_top_tokens = max(1, int(0.2 * len(sorted_indices)))  # Ensure at least 1 tokens is perturbed\n",
        "    top_indices_to_remove = sorted_indices[:n_top_tokens]\n",
        "\n",
        "    total_score_drop = 0\n",
        "\n",
        "    for i in range(1, n_top_tokens + 1):\n",
        "        # Perturb the input by removing the top i important tokens\n",
        "        indices_to_remove = top_indices_to_remove[:i]\n",
        "        perturbed_tokens = [tokens[j] for j in range(len(tokens)) if j not in indices_to_remove]\n",
        "        perturbed_text = \" \".join(perturbed_tokens)\n",
        "        print(indices_to_remove)\n",
        "\n",
        "        # Get new prediction for perturbed text\n",
        "        new_prediction = predict_proba([perturbed_text])[0]\n",
        "\n",
        "        # Calculate the drop in prediction score for the predicted class\n",
        "        score_drop = original_prediction[predicted_class_idx] - new_prediction[predicted_class_idx]\n",
        "        total_score_drop += score_drop\n",
        "        print(score_drop)\n",
        "\n",
        "    # Clean up memory\n",
        "    #del lime_values, tokens\n",
        "    #gc.collect()  # Trigger garbage collection to free up memory\n",
        "\n",
        "    # Calculate AOPC for the predicted class using only the top 20% of important tokens\n",
        "    aopc = total_score_drop / n_top_tokens\n",
        "    return attributions_scores, aopc"
      ],
      "metadata": {
        "id": "AcLeHSDlRP_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Loop: Calculate AOPC for Test Set, Export Results to a CSV file, and Print Group-Wise Mean AOPC by Score"
      ],
      "metadata": {
        "id": "8_mFyk4Z7qAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Loop: Calculate AOPC for Test Set ---\n",
        "\n",
        "aopc_list = []\n",
        "attributions_scores_list = []\n",
        "for idx, ex in enumerate(test_doc[0:5]):\n",
        "    print(f\"Processing example {idx+1} of {len(test_doc[0:5])}\")\n",
        "    attributions_scores, aopc = compute_aopc_lime(ex)\n",
        "    attributions_scores_list.append(attributions_scores)\n",
        "    aopc_list.append(aopc)\n",
        "\n",
        "# Report Mean AOPC\n",
        "print('Average AOPC to report: ', np.mean(aopc_list))\n",
        "\n",
        "# --- Save Results ---\n",
        "# Make sure 'essay_score' column exists in your CSV.\n",
        "output_df = pd.DataFrame({\n",
        "    'response': test_doc[0:5],\n",
        "    'attributions_scores': attributions_scores_list,\n",
        "    'lime_aopc': aopc_list,\n",
        "    'original_score': test['essay_score'][0:5] if 'essay_score' in test.columns else np.nan\n",
        "})\n",
        "\n",
        "output_df.to_csv('lime_aopc.csv', index=False)\n",
        "\n",
        "# Print group-wise mean AOPC by score (if available)\n",
        "if 'essay_score' in test.columns:\n",
        "    print(output_df.groupby('original_score')['lime_aopc'].mean())"
      ],
      "metadata": {
        "id": "7Wh88yGIWzt_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}